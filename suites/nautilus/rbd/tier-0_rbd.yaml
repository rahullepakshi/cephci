tests:
   - test:
      name: install ceph pre-requisites
      module: install_prereq.py
      abort-on-fail: true

   - test:
      name: ceph ansible
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: nautilus
            ceph_repository: rhcs
            osd_scenario: lvm
            osd_auto_discovery: False
            ceph_stable: True
            ceph_stable_rh_storage: True
            fetch_directory: ~/fetch
            copy_admin_key: true
            dashboard_enabled: False
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: setup four node cluster using ceph ansible
      destroy-cluster: false
      abort-on-fail: true

   - test:
      config:
        branch: nautilus
        script_path: qa/workunits/rbd
        script: cli_generic.sh
      desc: Executig upstream RBD CLI Generic scenarios
      module: test_rbd.py
      name: 1_rbd_cli_generic
      polarion-id: CEPH-83574241

   - test:
      config:
        branch: nautilus
        script_path: qa/workunits/rbd
        script: rbd_groups.sh
      desc: Executig upstream RBD CLI Groups scenarios
      module: test_rbd.py
      name: 2_rbd_cli_groups
      polarion-id: CEPH-83574239

   - test:
      config:
        branch: nautilus
        script_path: qa/workunits/rbd
        script: import_export.sh
      desc: Executig upstream RBD CLI Import Export scenarios
      module: test_rbd.py
      name: 3_rbd_cli_import_export
      polarion-id: CEPH-83574240

